\section{加分部分：TVM框架卷积计算优化}

\subsection{实验环境}

\begin{table}[H]
  \centering
  \begin{tabular}{c|c|c|c|c}
  \hline
  操作系统      & 架构   &  内存  & CPU & 内核版本       \\ \hline
  GNU/Linux & x86\_64 &   8GB  & 2CPU，12核心，24线程  & 5.8.0-53  \\ \hline
  \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
    \hline
    编程语言   & 解释器版本    & TVM框架版本      & LLVM版本   \\ \hline
    Python & 3.8.0 (CPython实现) & 0.8.dev0  & 12.0 \\ \hline
    \end{tabular}
  \end{table}

\subsection{实验设计}

为了执行更加准确的测试，并匹配当前使用的0.8.dev0版TVM，我们修改了原有的脚本，以获取更直观的输出。

\begin{lstlisting}[caption=测试组件示例,language=python]
def main():
    test_suite([1, 3, 32, 32, 32, 3, 3], 1)
    test_suite([100, 512, 32, 32, 1024, 3, 3], 2)
\end{lstlisting}


\subsubsection{小输入部分优化~原始计算内容分析}

调用tvm默认的schedule函数，我们能够得到以下运算的循环表达式。

\begin{lstlisting}[caption=原始循环表达式，在test1.old.ir文件中]
primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(float32), float32, [1, 32, 32, 32], []),
             B: Buffer(B_2: Pointer(float32), float32, [32, 3, 3, 3], []),
             A: Buffer(A_2: Pointer(float32), float32, [1, 3, 32, 32], [])}
  buffer_map = {A_1: A, B_1: B, compute_1: compute} {
  attr [pad_temp: Pointer(float32)] "storage_scope" = "global";
  allocate(pad_temp, float32, [3468]) {
    for (i1: int32, 0, 3) {
      for (i2: int32, 0, 34) {
        for (i3: int32, 0, 34) {
          pad_temp[(((i1*1156) + (i2*34)) + i3)] = @tir.if_then_else(((((1 <= i2) && (i2 < 33)) && (1 <= i3)) && (i3 < 33)), (float32*)A_2[((((i1*1024) + (i2*32)) + i3) - 33)], 0f32, dtype=float32)
        }
      }
    }
    for (ff: int32, 0, 32) {
      for (yy: int32, 0, 32) {
        for (xx: int32, 0, 32) {
          compute_2[(((ff*1024) + (yy*32)) + xx)] = 0f32
          for (rc: int32, 0, 3) {
            for (ry: int32, 0, 3) {
              for (rx: int32, 0, 3) {
                compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[(((((rc*1156) + (yy*34)) + (ry*34)) + xx) + rx)]*(float32*)B_2[((((ff*27) + (rc*9)) + (ry*3)) + rx)]))
              }
            }
          }
        }
      }
    }
  }
}
\end{lstlisting}

运行的时间是0.104413 ms。运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{1-1}}。可以看到还有很大优化空间。

其中，前一部分pad\_temp是卷积过程中的Padding。通常为了更好地节省内存和缓存占用，我们会将这一部分内联到后续运算中。但TVM提供的卷积包装器并不方便指向内部的阶段，我们此处不采用这种优化手段。有文章指出，对于深度学习中常用的卷积结构，通过likely标记判断过程，这样的优化可以提升约10\%的性能。

对于主要的运算过程compute\_2，不难发现，不经优化的原始运算内容是一次卷积，编译后会在每个要计算的图片上，逐位移动覆盖的卷积核并计算卷积值。编译后的计算表现为一次三重循环（初始化）与一次六重循环（赋值），每次计算一组赋值。在本次优化中，我们仅考虑对六重循环这一主要耗时部分进行优化。

\subsubsection{卷积核循环展开}

我们通过以下的schedule函数来展开卷积核：

\begin{lstlisting}[caption=卷积核循环展开,language=python]
def schedule(output):
    s = tvm.te.create_schedule(output.op)
    s[output].unroll(output.op.reduce_axis[2])
    s[output].unroll(output.op.reduce_axis[1])
    return s
\end{lstlisting}

可以得到新的循环表达式：

\begin{lstlisting}[caption=卷积核展开后的循环表达式]
for (ff: int32, 0, 32) {
  for (yy: int32, 0, 32) {
    for (xx: int32, 0, 32) {
      compute_2[(((ff*1024) + (yy*32)) + xx)] = 0f32
      for (rc: int32, 0, 3) {
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[(((rc*1156) + (yy*34)) + xx)]*(float32*)B_2[((ff*27) + (rc*9))]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 1)]*(float32*)B_2[(((ff*27) + (rc*9)) + 1)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 2)]*(float32*)B_2[(((ff*27) + (rc*9)) + 2)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 34)]*(float32*)B_2[(((ff*27) + (rc*9)) + 3)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 35)]*(float32*)B_2[(((ff*27) + (rc*9)) + 4)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 36)]*(float32*)B_2[(((ff*27) + (rc*9)) + 5)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 68)]*(float32*)B_2[(((ff*27) + (rc*9)) + 6)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 69)]*(float32*)B_2[(((ff*27) + (rc*9)) + 7)]))
        compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[((((rc*1156) + (yy*34)) + xx) + 70)]*(float32*)B_2[(((ff*27) + (rc*9)) + 8)]))
      }
    }
  }
}
\end{lstlisting}

此部分的运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{1-2}}。

可以发现，现在每次最内循环都是多次赋值。这样，我们可以减少循环次数，并为接下来的优化打下基础。

\subsubsection{vectorize原语进行向量化计算}

在经过unroll处理后，我们发现每次内层循环的举动都是一系列float32的赋值过程。这一过程中指令相同数据不同，非常适合利用CPU提供的SIMD指令集。因此我们对内层运算添加指令s[output].vectorize(output.op.axis[3])，让内层的赋值运算变为向量形式。运行结果如下：

\begin{lstlisting}[caption=向量化计算后的循环表达式]
for (ff: int32, 0, 32) {
    for (yy: int32, 0, 32) {
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = broadcast(0f32, 32)
      for (rc: int32, 0, 3) {
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp(((rc*1156) + (yy*34)), 1, 32)]*broadcast((float32*)B_2[((ff*27) + (rc*9))], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 1), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 1)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 2), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 2)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 34), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 3)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 35), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 4)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 36), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 5)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 68), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 6)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 69), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 7)], 32)))
            compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 70), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 8)], 32)))
      }
    }
}
\end{lstlisting}

运行时间是0.056781 ms，运算速度有了较大提升。运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{1-3}}。

\subsubsection{parallel原语并行执行内部循环}

经过向量化指令，运行速度已经有了较大的提高。但是因为当前运算规模相对较小，我们再对for循环结构进行优化收益已经不大，因此我们考虑增加系统同时运行的计算数量。幸运的是，卷积运行模型中，各个外层运算式中没有数据相关性，因此我们可以放心地添加并行指令。对函数添加指令`s[output].parallel(output.op.axis[1])`，让最外层的for循环变为并行执行。

到此，我们的schedule函数变为

\begin{lstlisting}[caption=小输入部分优化函数,language=python]
def schedule(output):
    s = tvm.te.create_schedule(output.op)
    s[output].parallel(output.op.axis[1])
    s[output].vectorize(output.op.axis[3])
    s[output].unroll(output.op.reduce_axis[2])
    s[output].unroll(output.op.reduce_axis[1])
    return s
\end{lstlisting}

运行结果如下：

\begin{lstlisting}[caption=并行执行后的循环表达式]
for (ff: int32, 0, 32) "parallel" {
  for (yy: int32, 0, 32) {
    compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = broadcast(0f32, 32)
    for (rc: int32, 0, 3) {
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp(((rc*1156) + (yy*34)), 1, 32)]*broadcast((float32*)B_2[((ff*27) + (rc*9))], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 1), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 1)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 2), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 2)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 34), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 3)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 35), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 4)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 36), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 5)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 68), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 6)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 69), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 7)], 32)))
      compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 70), 1, 32)]*broadcast((float32*)B_2[(((ff*27) + (rc*9)) + 8)], 32)))
    }
  }
}
\end{lstlisting}

运行时间是0.016186 ms。运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{1-4}}。可以发现，由于实验系统允许以较多的线程并行执行，并行的增益效果非常显著。

至此，我们完成了对小规模数组的优化工作，优化性能达到了91.2\%。

\subsubsection{大输入部分优化}

调用tvm默认的schedule函数，我们能够得到以下运算的循环表达式。

\begin{lstlisting}[caption=大输入原始循环表达式，在test2.old.ir文件中]
primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()
  attr = {"global_symbol": "main", "tir.noalias": True}
  buffers = {compute: Buffer(compute_2: Pointer(float32), float32, [1, 1024, 32, 32], []),
             B: Buffer(B_2: Pointer(float32), float32, [1024, 512, 3, 3], []),
             A: Buffer(A_2: Pointer(float32), float32, [1, 512, 32, 32], [])}
  buffer_map = {A_1: A, B_1: B, compute_1: compute} {
  attr [pad_temp: Pointer(float32)] "storage_scope" = "global";
  allocate(pad_temp, float32, [591872]) {
    for (i1: int32, 0, 512) {
      for (i2: int32, 0, 34) {
        for (i3: int32, 0, 34) {
          pad_temp[(((i1*1156) + (i2*34)) + i3)] = @tir.if_then_else(((((1 <= i2) && (i2 < 33)) && (1 <= i3)) && (i3 < 33)), (float32*)A_2[((((i1*1024) + (i2*32)) + i3) - 33)], 0f32, dtype=float32)
        }
      }
    }
    for (ff: int32, 0, 1024) {
      for (yy: int32, 0, 32) {
        for (xx: int32, 0, 32) {
          compute_2[(((ff*1024) + (yy*32)) + xx)] = 0f32
          for (rc: int32, 0, 512) {
            for (ry: int32, 0, 3) {
              for (rx: int32, 0, 3) {
                compute_2[(((ff*1024) + (yy*32)) + xx)] = ((float32*)compute_2[(((ff*1024) + (yy*32)) + xx)] + ((float32*)pad_temp[(((((rc*1156) + (yy*34)) + (ry*34)) + xx) + rx)]*(float32*)B_2[((((ff*4608) + (rc*9)) + (ry*3)) + rx)]))
              }
            }
          }
        }
      }
    }
  }
}
\end{lstlisting}

运行时间是505076.882729 ms。运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{2-1}}。可以看到还有很大优化空间。

我们沿用小输入优化的schedule函数（图\textcolor{MidnightBlue}{\ref{2-2}}），能够得到一个较好的结果，这已经达到了优化的要求。

但是我们发现，卷积核的filter较大。因为filter是一个线性的组成，如果我们能够将filter调换顺序，把大的filter调换到外部，可以增加程序的局部性。

\begin{lstlisting}[caption=最终优化函数,language=python]
def schedule(output):
    s = tvm.te.create_schedule(output.op)
    if output.op.reduce_axis[0].dom.extent > output.op.axis[3].dom.extent:
        s[output].reorder(output.op.axis[0], output.op.axis[1], output.op.reduce_axis[0], output.op.axis[2],
                          output.op.axis[3])
    s[output].parallel(output.op.axis[1])
    s[output].vectorize(output.op.axis[3])
    s[output].unroll(output.op.reduce_axis[2])
    s[output].unroll(output.op.reduce_axis[1])
    return s
\end{lstlisting}

运行结果如下：

\begin{lstlisting}[caption=大输入优化循环表达式，在test2.opt.ir文件中]
for (ff: int32, 0, 1024) "parallel" {
    for (yy.init: int32, 0, 32) {
    compute_2[ramp(((ff*1024) + (yy.init*32)), 1, 32)] = broadcast(0f32, 32)
    }
    for (rc: int32, 0, 512) {
    for (yy: int32, 0, 32) {
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp(((rc*1156) + (yy*34)), 1, 32)]*broadcast((float32*)B_2[((ff*4608) + (rc*9))], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 1), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 1)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 2), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 2)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 34), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 3)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 35), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 4)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 36), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 5)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 68), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 6)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 69), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 7)], 32)))
        compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] = ((float32x32*)compute_2[ramp(((ff*1024) + (yy*32)), 1, 32)] + ((float32x32*)pad_temp[ramp((((rc*1156) + (yy*34)) + 70), 1, 32)]*broadcast((float32*)B_2[(((ff*4608) + (rc*9)) + 8)], 32)))
    }
    }
}
\end{lstlisting}

运行结果参见附录中的图片\textcolor{MidnightBlue}{\ref{2-3}}。

此时我们的运行时间是16044.837527 ms，优化性能达到了96.8\%。
